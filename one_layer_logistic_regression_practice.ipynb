{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahlia52/Advanced-Statistical-Data-Analysis/blob/main/one_layer_logistic_regression_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6MJv9kYSxTUx"
      },
      "id": "6MJv9kYSxTUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d2fe3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7d2fe3e",
        "outputId": "c84d70a8-ca25-44ea-f04e-086bcbc3341f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3)\n"
          ]
        }
      ],
      "source": [
        "num_sample = 500\n",
        "\n",
        "data0 = np.random.randn(num_sample,2) + (2,2)\n",
        "data1 = np.random.randn(num_sample,2) + (-2,-2)\n",
        "\n",
        "data0 = np.hstack([data0,np.zeros((num_sample,1),dtype=float)])\n",
        "data1 = np.hstack([data1,np.ones((num_sample,1),dtype=float)])\n",
        "\n",
        "data = np.vstack([data0,data1])\n",
        "\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3sN8oOlxrU9",
        "outputId": "d4d270d7-b8ac-4970-a89c-f050db14607a"
      },
      "id": "p3sN8oOlxrU9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.236788  ,  2.14095442,  0.        ],\n",
              "       [ 3.12806048,  2.92277511,  0.        ],\n",
              "       [ 2.08697688,  1.48163492,  0.        ],\n",
              "       ...,\n",
              "       [-2.82204399, -0.77232958,  1.        ],\n",
              "       [-0.50074832, -1.9921468 ,  1.        ],\n",
              "       [-1.79009761, -2.38530759,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11002ec5",
      "metadata": {
        "id": "11002ec5"
      },
      "outputs": [],
      "source": [
        "# we are assuming one layer logistic regression\n",
        "# w1*x1 + w2*x2 + b = XW + b\n",
        "w = np.random.randn(2,1) # w1, w2\n",
        "b = np.random.randn(1,1)  # scalar\n",
        "eta = 1e-5 # learning rate\n",
        "delta = 1e-10 # prevent log 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4b0ade",
      "metadata": {
        "id": "ca4b0ade"
      },
      "outputs": [],
      "source": [
        "# define sigmoid function\n",
        "def sigmoid(val):\n",
        "    result = 1 / (1+np.exp(-val))\n",
        "    return result\n",
        "\n",
        "# define derivative of sigmoid function w.r.t. its value\n",
        "def grad_sigmoid(val):\n",
        "    result = sigmoid(val)\n",
        "    result *= (1-result)\n",
        "    return result\n",
        "\n",
        "# given data instances in batch form,\n",
        "# compute loss and gradients of w and b\n",
        "# also, count the number of correct prediction\n",
        "def compute_loss_and_grad(data_instance):\n",
        "    x, y = data_instance\n",
        "    linear = np.matmul(x,w) + b\n",
        "    y_est = sigmoid(linear)\n",
        "\n",
        "    # loss\n",
        "    # 로그의 진수 부분을 0이 아닌 양수가 되도록 하기 위해 아주 작은 값 delta를 더해줌.\n",
        "    loss = -y*np.log(y_est + delta) - (1-y)*np.log(1-y_est + delta)\n",
        "\n",
        "    # dL/dh = dL/dt * dt/dh\n",
        "    grad = -y*(1-y_est) + (1-y)*y_est\n",
        "\n",
        "    # dL/dw = dL/dt * dt/dh * dh/dx\n",
        "    # elementwise product (a,b), (c,d) -> (ac, bd)\n",
        "    # grad.shape = (1,1), x.shape = (2,) -> broadcasting (np.matmul에서는 오류 발생)\n",
        "    grad_w = np.multiply(grad,x)\n",
        "\n",
        "    # dL/db = dL/dt * dt/dh * dh/db\n",
        "    grad_b = grad\n",
        "\n",
        "    hit = (y == np.round(y_est)) # 예측이 맞는가\n",
        "\n",
        "    return loss, (grad_w, grad_b), hit\n",
        "\n",
        "# update NN parameters w and b with SGD\n",
        "def update_parameters(params,grads):\n",
        "    w, b = params\n",
        "    grad_w, grad_b = grads\n",
        "\n",
        "    w -= eta * grad_w.reshape(-1,1) # w.shape = (2,1), grad_w.shape = (1,2)\n",
        "    b -= eta * grad_b\n",
        "\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9591e77c",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9591e77c",
        "outputId": "a2a7b068-16e9-4c48-d5c9-1d44d4badcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train: 0.684 / 61.90 %\n",
            "Epoch 5 Train: 0.625 / 66.10 %\n",
            "Epoch 10 Train: 0.573 / 69.00 %\n",
            "Epoch 15 Train: 0.528 / 72.40 %\n",
            "Epoch 20 Train: 0.488 / 75.00 %\n",
            "Epoch 25 Train: 0.453 / 77.10 %\n",
            "Epoch 30 Train: 0.422 / 79.40 %\n",
            "Epoch 35 Train: 0.394 / 81.30 %\n",
            "Epoch 40 Train: 0.370 / 83.30 %\n",
            "Epoch 45 Train: 0.348 / 84.70 %\n",
            "Epoch 50 Train: 0.328 / 85.70 %\n",
            "Epoch 55 Train: 0.311 / 86.40 %\n",
            "Epoch 60 Train: 0.295 / 87.40 %\n",
            "Epoch 65 Train: 0.280 / 88.70 %\n",
            "Epoch 70 Train: 0.267 / 89.60 %\n",
            "Epoch 75 Train: 0.255 / 90.90 %\n",
            "Epoch 80 Train: 0.244 / 91.60 %\n",
            "Epoch 85 Train: 0.234 / 92.00 %\n",
            "Epoch 90 Train: 0.225 / 92.30 %\n",
            "Epoch 95 Train: 0.216 / 93.00 %\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 100\n",
        "\n",
        "# train the logistic regression model\n",
        "for i in range(num_epoch):\n",
        "    # shuffle traning data by permutation\n",
        "    perm = np.random.permutation(len(data)) # shuffling the index of data\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for j in range(len(data)):\n",
        "        # feed data instances one-by-one, i.e., mini-batch size is 1\n",
        "        x = data[perm[j]][:-1]\n",
        "        y = data[perm[j]][-1]# (1,1)\n",
        "        y = y.reshape([1])# (1,)\n",
        "        params = (w, b)\n",
        "        # compute loss and gradients, and then update the parameters\n",
        "        # also, compute sum of the loss and the number of correct prediction\n",
        "        loss, grads, hit = compute_loss_and_grad((x,y))\n",
        "        w, b = update_parameters(params, grads)\n",
        "        total_loss += loss.sum() # (1,1) -> scalar\n",
        "        count += hit.sum()\n",
        "\n",
        "    # compute average loss and accuracy for the train dataset\n",
        "    loss_train = total_loss / len(data)\n",
        "    acc_train = count / len(data)\n",
        "\n",
        "    if i % 5 == 0:\n",
        "        print(\"Epoch %d Train: %.3f / %.2f %%\"%(i,loss_train,acc_train*100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}