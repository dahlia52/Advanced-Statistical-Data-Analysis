{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahlia52/Advanced-Statistical-Data-Analysis/blob/main/LeNet_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFZrRhaNRqOv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liUistX7Bwv1",
        "outputId": "60e2e9dc-e8d7-4710-8718-984f19f3ff3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 93014701.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 88420214.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 21142790.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16755082.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST/raw\n",
            "\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "torch.Size([1, 28, 28]) 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()]) # 이미지를 텐서로 변환\n",
        "\n",
        "# Prepare Data\n",
        "train_data = MNIST(root = path, train = True, transform = transform, download = True)\n",
        "test_data = MNIST(root = path, train = False, transform = transform, download = True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = False, num_workers = 4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "input_shape = train_data[0][0].shape # (1,28,28)\n",
        "output_shape = len(train_data.classes)\n",
        "\n",
        "print(input_shape,output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvmVJ0hxjkLD",
        "outputId": "b9b53082-be04-4f5c-9fdb-e780644d7205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu:0\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = (5,5), stride = 1, padding = 2) # MNIST 데이터는 흑백 데이터이므로 in_channels = 1\n",
        "    self.pool1 = nn.AvgPool2d(kernel_size = (2,2), stride = 2, padding = 0)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = (5,5), stride = 1) # kernel의 가로, 세로 길이가 동일하므로 kernel_size = 5도 가능\n",
        "    self.pool2 = nn.AvgPool2d(kernel_size = (2,2), stride = 2, padding = 0)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(400,120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,output_shape)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(x) # (100,1,28,28) # batch_size = 100\n",
        "    hidden = F.leaky_relu(self.conv1(x))\n",
        "    # print(hidden.shape) # (100,6,28,28)\n",
        "    hidden = self.pool1(hidden)\n",
        "    # print(hidden.shape) # (100,6,14,14)\n",
        "    hidden = F.leaky_relu(self.conv2(hidden))\n",
        "    # print(hidden.shape) # (100,16,10,10)\n",
        "    hidden = self.pool2(hidden)\n",
        "    # print(hidden.shape) # (100,16,5,5)\n",
        "    hidden = self.flatten(hidden)\n",
        "    # print(hidden.shape) # (100,400)\n",
        "    hidden = F.leaky_relu(self.fc1(hidden))\n",
        "    # print(hidden.shape) # (100,120)\n",
        "    hidden = F.leaky_relu(self.fc2(hidden))\n",
        "    # print(hidden.shape) # (100,84)\n",
        "    output = self.fc3(hidden)\n",
        "    # print(output.shape) # (100,10)\n",
        "    return output"
      ],
      "metadata": {
        "id": "An9qwWS7MhaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet().to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)"
      ],
      "metadata": {
        "id": "9-V8mQx-xZEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 15\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "  # train\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    y_est = model.forward(x)\n",
        "    cost = loss(y_est, y)\n",
        "\n",
        "    total_loss += cost.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = torch.argmax(y_est, dim = -1)\n",
        "    count += (pred == y).sum().item()\n",
        "\n",
        "  acc = count / len(train_data)\n",
        "  avg_loss = total_loss / len(train_data)\n",
        "\n",
        "  train_loss_list.append(avg_loss)\n",
        "\n",
        "  if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x, y) in enumerate(test_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_est = model.forward(x)\n",
        "      cost = loss(y_est, y)\n",
        "\n",
        "      total_loss += cost.item()\n",
        "\n",
        "      pred = torch.argmax(y_est, dim = -1)\n",
        "      count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count / len(test_data)\n",
        "    avg_loss = total_loss / len(test_data)\n",
        "\n",
        "    test_loss_list.append(avg_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      print(\"Epoch %d Test: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o6hk0nXxwna",
        "outputId": "b3292421-4cec-4761-f5b2-27e13399e142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train: Loss 0.006 / Accuracy 0.998\n",
            "Epoch 0 Test: Loss 0.035 / Accuracy 0.991\n",
            "\n",
            "Epoch 1 Train: Loss 0.006 / Accuracy 0.998\n",
            "Epoch 1 Test: Loss 0.036 / Accuracy 0.990\n",
            "\n",
            "Epoch 2 Train: Loss 0.006 / Accuracy 0.998\n",
            "Epoch 2 Test: Loss 0.036 / Accuracy 0.991\n",
            "\n",
            "Epoch 3 Train: Loss 0.006 / Accuracy 0.998\n",
            "Epoch 3 Test: Loss 0.040 / Accuracy 0.990\n",
            "\n",
            "Epoch 4 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 4 Test: Loss 0.035 / Accuracy 0.992\n",
            "\n",
            "Epoch 5 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 5 Test: Loss 0.042 / Accuracy 0.991\n",
            "\n",
            "Epoch 6 Train: Loss 0.005 / Accuracy 0.999\n",
            "Epoch 6 Test: Loss 0.031 / Accuracy 0.992\n",
            "\n",
            "Epoch 7 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 7 Test: Loss 0.031 / Accuracy 0.992\n",
            "\n",
            "Epoch 8 Train: Loss 0.005 / Accuracy 0.998\n",
            "Epoch 8 Test: Loss 0.059 / Accuracy 0.989\n",
            "\n",
            "Epoch 9 Train: Loss 0.006 / Accuracy 0.998\n",
            "Epoch 9 Test: Loss 0.039 / Accuracy 0.991\n",
            "\n",
            "Epoch 10 Train: Loss 0.005 / Accuracy 0.999\n",
            "Epoch 10 Test: Loss 0.042 / Accuracy 0.991\n",
            "\n",
            "Epoch 11 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 11 Test: Loss 0.037 / Accuracy 0.992\n",
            "\n",
            "Epoch 12 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 12 Test: Loss 0.039 / Accuracy 0.990\n",
            "\n",
            "Epoch 13 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 13 Test: Loss 0.045 / Accuracy 0.990\n",
            "\n",
            "Epoch 14 Train: Loss 0.004 / Accuracy 0.999\n",
            "Epoch 14 Test: Loss 0.041 / Accuracy 0.991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_parameter = 0\n",
        "\n",
        "for parameter in model.parameters():\n",
        "  print(parameter.shape)\n",
        "  num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zc-o4L48DvL",
        "outputId": "d4feb9b1-32f2-47ce-9943-61518efab59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 1, 5, 5])\n",
            "torch.Size([6])\n",
            "torch.Size([16, 6, 5, 5])\n",
            "torch.Size([16])\n",
            "torch.Size([120, 400])\n",
            "torch.Size([120])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([84])\n",
            "torch.Size([10, 84])\n",
            "torch.Size([10])\n",
            "61706\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOACS4+O6bOR/8Eqx872P96",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}