{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahlia52/Advanced-Statistical-Data-Analysis/blob/main/softmax_classification_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JFZrRhaNRqOv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liUistX7Bwv1",
        "outputId": "b417fcf2-605c-4948-e599-a6148358e66d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 87788813.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./datasets/cifar-100-python.tar.gz to ./datasets/\n",
            "Files already downloaded and verified\n",
            "Dataset CIFAR100\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset CIFAR100\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "3072 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()]) # 이미지를 텐서로 변환\n",
        "\n",
        "# Prepare Data\n",
        "train_data = CIFAR100(root = path, train = True, transform = transform, download = True)\n",
        "test_data = CIFAR100(root = path, train = False, transform = transform, download = True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = False, num_workers = 4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "input_shape = train_data[0][0].reshape(-1).shape[0] # 3072 = 3*32*32\n",
        "output_shape = len(train_data.classes) # 100\n",
        "\n",
        "print(input_shape,output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvmVJ0hxjkLD",
        "outputId": "89099859-d330-4866-8a7c-94af31ea9212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu:0\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import LeakyReLU\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(input_shape, 1728),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(1728, 512),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(128, output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "AJDgNCfsSr6K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SoftmaxClassifier().to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)"
      ],
      "metadata": {
        "id": "9-V8mQx-xZEl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 50\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "  # train\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    y_est = model.forward(x)\n",
        "    cost = loss(y_est, y)\n",
        "\n",
        "    total_loss += cost.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = torch.argmax(y_est, dim = -1)\n",
        "    count += (pred == y).sum().item()\n",
        "\n",
        "  acc = count / len(train_data)\n",
        "  avg_loss = total_loss / len(train_data)\n",
        "\n",
        "  train_loss_list.append(avg_loss)\n",
        "\n",
        "  if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (x, y) in enumerate(test_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_est = model.forward(x)\n",
        "      cost = loss(y_est, y)\n",
        "\n",
        "      total_loss += cost.item()\n",
        "\n",
        "      pred = torch.argmax(y_est, dim = -1)\n",
        "      count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count / len(test_data)\n",
        "    avg_loss = total_loss / len(test_data)\n",
        "\n",
        "    test_loss_list.append(avg_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      print(\"Epoch %d Test: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o6hk0nXxwna",
        "outputId": "1aaf9ce4-05b2-443e-a3af-f4235aa7b8bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train: Loss 4.609 / Accuracy 0.011\n",
            "Epoch 0 Test: Loss 4.581 / Accuracy 0.014\n",
            "\n",
            "Epoch 1 Train: Loss 4.450 / Accuracy 0.023\n",
            "Epoch 1 Test: Loss 4.296 / Accuracy 0.047\n",
            "\n",
            "Epoch 2 Train: Loss 4.303 / Accuracy 0.036\n",
            "Epoch 2 Test: Loss 4.167 / Accuracy 0.055\n",
            "\n",
            "Epoch 3 Train: Loss 4.234 / Accuracy 0.044\n",
            "Epoch 3 Test: Loss 4.136 / Accuracy 0.063\n",
            "\n",
            "Epoch 4 Train: Loss 4.186 / Accuracy 0.050\n",
            "Epoch 4 Test: Loss 4.077 / Accuracy 0.067\n",
            "\n",
            "Epoch 5 Train: Loss 4.166 / Accuracy 0.053\n",
            "Epoch 5 Test: Loss 4.042 / Accuracy 0.076\n",
            "\n",
            "Epoch 6 Train: Loss 4.143 / Accuracy 0.056\n",
            "Epoch 6 Test: Loss 4.024 / Accuracy 0.076\n",
            "\n",
            "Epoch 7 Train: Loss 4.124 / Accuracy 0.057\n",
            "Epoch 7 Test: Loss 4.013 / Accuracy 0.079\n",
            "\n",
            "Epoch 8 Train: Loss 4.111 / Accuracy 0.062\n",
            "Epoch 8 Test: Loss 3.988 / Accuracy 0.091\n",
            "\n",
            "Epoch 9 Train: Loss 4.098 / Accuracy 0.063\n",
            "Epoch 9 Test: Loss 3.992 / Accuracy 0.087\n",
            "\n",
            "Epoch 10 Train: Loss 4.084 / Accuracy 0.066\n",
            "Epoch 10 Test: Loss 3.958 / Accuracy 0.094\n",
            "\n",
            "Epoch 11 Train: Loss 4.080 / Accuracy 0.066\n",
            "Epoch 11 Test: Loss 3.949 / Accuracy 0.090\n",
            "\n",
            "Epoch 12 Train: Loss 4.070 / Accuracy 0.065\n",
            "Epoch 12 Test: Loss 3.965 / Accuracy 0.089\n",
            "\n",
            "Epoch 13 Train: Loss 4.058 / Accuracy 0.068\n",
            "Epoch 13 Test: Loss 3.941 / Accuracy 0.094\n",
            "\n",
            "Epoch 14 Train: Loss 4.043 / Accuracy 0.071\n",
            "Epoch 14 Test: Loss 3.916 / Accuracy 0.095\n",
            "\n",
            "Epoch 15 Train: Loss 4.037 / Accuracy 0.072\n",
            "Epoch 15 Test: Loss 3.904 / Accuracy 0.095\n",
            "\n",
            "Epoch 16 Train: Loss 4.028 / Accuracy 0.071\n",
            "Epoch 16 Test: Loss 3.907 / Accuracy 0.104\n",
            "\n",
            "Epoch 17 Train: Loss 4.021 / Accuracy 0.074\n",
            "Epoch 17 Test: Loss 3.893 / Accuracy 0.105\n",
            "\n",
            "Epoch 18 Train: Loss 4.015 / Accuracy 0.076\n",
            "Epoch 18 Test: Loss 3.866 / Accuracy 0.104\n",
            "\n",
            "Epoch 19 Train: Loss 4.012 / Accuracy 0.075\n",
            "Epoch 19 Test: Loss 3.908 / Accuracy 0.103\n",
            "\n",
            "Epoch 20 Train: Loss 4.013 / Accuracy 0.076\n",
            "Epoch 20 Test: Loss 3.902 / Accuracy 0.103\n",
            "\n",
            "Epoch 21 Train: Loss 4.000 / Accuracy 0.077\n",
            "Epoch 21 Test: Loss 3.855 / Accuracy 0.105\n",
            "\n",
            "Epoch 22 Train: Loss 3.998 / Accuracy 0.077\n",
            "Epoch 22 Test: Loss 3.855 / Accuracy 0.108\n",
            "\n",
            "Epoch 23 Train: Loss 3.990 / Accuracy 0.079\n",
            "Epoch 23 Test: Loss 3.840 / Accuracy 0.112\n",
            "\n",
            "Epoch 24 Train: Loss 3.985 / Accuracy 0.080\n",
            "Epoch 24 Test: Loss 3.891 / Accuracy 0.102\n",
            "\n",
            "Epoch 25 Train: Loss 3.977 / Accuracy 0.081\n",
            "Epoch 25 Test: Loss 3.848 / Accuracy 0.109\n",
            "\n",
            "Epoch 26 Train: Loss 3.971 / Accuracy 0.082\n",
            "Epoch 26 Test: Loss 3.830 / Accuracy 0.114\n",
            "\n",
            "Epoch 27 Train: Loss 3.972 / Accuracy 0.080\n",
            "Epoch 27 Test: Loss 3.818 / Accuracy 0.120\n",
            "\n",
            "Epoch 28 Train: Loss 3.958 / Accuracy 0.083\n",
            "Epoch 28 Test: Loss 3.813 / Accuracy 0.118\n",
            "\n",
            "Epoch 29 Train: Loss 3.957 / Accuracy 0.084\n",
            "Epoch 29 Test: Loss 3.802 / Accuracy 0.113\n",
            "\n",
            "Epoch 30 Train: Loss 3.956 / Accuracy 0.083\n",
            "Epoch 30 Test: Loss 3.824 / Accuracy 0.123\n",
            "\n",
            "Epoch 31 Train: Loss 3.955 / Accuracy 0.085\n",
            "Epoch 31 Test: Loss 3.843 / Accuracy 0.116\n",
            "\n",
            "Epoch 32 Train: Loss 3.949 / Accuracy 0.086\n",
            "Epoch 32 Test: Loss 3.812 / Accuracy 0.115\n",
            "\n",
            "Epoch 33 Train: Loss 3.949 / Accuracy 0.086\n",
            "Epoch 33 Test: Loss 3.802 / Accuracy 0.119\n",
            "\n",
            "Epoch 34 Train: Loss 3.948 / Accuracy 0.087\n",
            "Epoch 34 Test: Loss 3.782 / Accuracy 0.122\n",
            "\n",
            "Epoch 35 Train: Loss 3.932 / Accuracy 0.089\n",
            "Epoch 35 Test: Loss 3.810 / Accuracy 0.115\n",
            "\n",
            "Epoch 36 Train: Loss 3.936 / Accuracy 0.088\n",
            "Epoch 36 Test: Loss 3.777 / Accuracy 0.121\n",
            "\n",
            "Epoch 37 Train: Loss 3.923 / Accuracy 0.090\n",
            "Epoch 37 Test: Loss 3.780 / Accuracy 0.117\n",
            "\n",
            "Epoch 38 Train: Loss 3.930 / Accuracy 0.089\n",
            "Epoch 38 Test: Loss 3.767 / Accuracy 0.129\n",
            "\n",
            "Epoch 39 Train: Loss 3.929 / Accuracy 0.088\n",
            "Epoch 39 Test: Loss 3.760 / Accuracy 0.124\n",
            "\n",
            "Epoch 40 Train: Loss 3.921 / Accuracy 0.090\n",
            "Epoch 40 Test: Loss 3.816 / Accuracy 0.116\n",
            "\n",
            "Epoch 41 Train: Loss 3.925 / Accuracy 0.091\n",
            "Epoch 41 Test: Loss 3.764 / Accuracy 0.129\n",
            "\n",
            "Epoch 42 Train: Loss 3.927 / Accuracy 0.090\n",
            "Epoch 42 Test: Loss 3.764 / Accuracy 0.126\n",
            "\n",
            "Epoch 43 Train: Loss 3.916 / Accuracy 0.089\n",
            "Epoch 43 Test: Loss 3.766 / Accuracy 0.127\n",
            "\n",
            "Epoch 44 Train: Loss 3.930 / Accuracy 0.091\n",
            "Epoch 44 Test: Loss 3.737 / Accuracy 0.134\n",
            "\n",
            "Epoch 45 Train: Loss 3.907 / Accuracy 0.092\n",
            "Epoch 45 Test: Loss 3.747 / Accuracy 0.126\n",
            "\n",
            "Epoch 46 Train: Loss 3.902 / Accuracy 0.094\n",
            "Epoch 46 Test: Loss 3.758 / Accuracy 0.129\n",
            "\n",
            "Epoch 47 Train: Loss 3.906 / Accuracy 0.092\n",
            "Epoch 47 Test: Loss 3.740 / Accuracy 0.126\n",
            "\n",
            "Epoch 48 Train: Loss 3.900 / Accuracy 0.094\n",
            "Epoch 48 Test: Loss 3.702 / Accuracy 0.131\n",
            "\n",
            "Epoch 49 Train: Loss 3.893 / Accuracy 0.095\n",
            "Epoch 49 Test: Loss 3.752 / Accuracy 0.131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_parameter = 0\n",
        "\n",
        "for parameter in model.parameters():\n",
        "  print(parameter.shape)\n",
        "  num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "id": "2Zc-o4L48DvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365d295a-9b4e-4dd1-84b6-9a93ac72a32d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1728, 3072])\n",
            "torch.Size([1728])\n",
            "torch.Size([512, 1728])\n",
            "torch.Size([512])\n",
            "torch.Size([256, 512])\n",
            "torch.Size([256])\n",
            "torch.Size([128, 256])\n",
            "torch.Size([128])\n",
            "torch.Size([100, 128])\n",
            "torch.Size([100])\n",
            "6372516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 파라미터 개수는 더 많지만, LeNet보다 성능이 안좋음."
      ],
      "metadata": {
        "id": "JbUDPQO1WLB2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzcz+OL9zTQpC1lmi5urjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}