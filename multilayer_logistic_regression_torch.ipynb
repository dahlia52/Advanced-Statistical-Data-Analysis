{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2QR4kyThGZTa6Zoq77WWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahlia52/Advanced-Statistical-Data-Analysis/blob/main/multilayer_logistic_regression_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFW1S404giJo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_sample = 250\n",
        "\n",
        "# 데이터 생성\n",
        "data0 = np.random.randn(num_sample, 2) + (2,2)\n",
        "data1 = np.random.randn(num_sample, 2) + (-2,-2)\n",
        "data2 = np.random.randn(num_sample, 2) + (2,-2)\n",
        "data3 = np.random.randn(num_sample, 2) + (-2,2)\n",
        "\n",
        "data0 = np.hstack([data0, np.zeros((num_sample, 1), dtype = float)])\n",
        "data1 = np.hstack([data1, np.zeros((num_sample, 1), dtype = float)])\n",
        "data2 = np.hstack([data2, np.ones((num_sample, 1), dtype = float)])\n",
        "data3 = np.hstack([data3, np.ones((num_sample, 1), dtype = float)])\n",
        "\n",
        "data = np.vstack([data0, data1, data2, data3])\n",
        "\n",
        "data = torch.tensor(data, dtype = torch.float32) # data를 tensor로 변환\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMEpOnlQiyfE",
        "outputId": "bd6193c1-b95d-4580-837e-435529310e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # input.dims = 2 -> output.dims = 1\n",
        "    self.linear1 = nn.Linear(2,3)\n",
        "    self.linear2 = nn.Linear(3,3)\n",
        "    self.linear3 = nn.Linear(3,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # logit = self.linear(x)\n",
        "    # output = F.sigmoid(logit)\n",
        "    hidden1 = F.leaky_relu(self.linear1(x))\n",
        "    hidden2 = F.tanh(self.linear2(hidden1))\n",
        "    #output = F.sigmoid(self.linear3(hidden2)) # 0~1 -> log ... + delta\n",
        "    output = self.linear3(hidden2) # logit = inverse of sigmoid (softmax)\n",
        "    return output"
      ],
      "metadata": {
        "id": "B5zA9I1YkTuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "#loss = nn.BCELoss() # need sigmoid activation\n",
        "loss = nn.BCEWithLogitsLoss() # do not need sigmoid, only need (pre-sigmoid) logit value\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)"
      ],
      "metadata": {
        "id": "XL1kzMjHsG6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 200\n",
        "loss_list = list()"
      ],
      "metadata": {
        "id": "biuOxqivsWmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epoch):\n",
        "  perm = np.random.permutation(len(data))\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for j in range(len(data)):\n",
        "    x = data[perm[j]][:-1]\n",
        "    y = data[perm[j]][-1] # (1,1)\n",
        "\n",
        "    y = y.reshape([1]) # (1,)\n",
        "    #out = model.forward(x) # P(y=1lx)\n",
        "    logit = model.forward(x)\n",
        "    prob = F.sigmoid(logit)\n",
        "\n",
        "    #cost = loss(out, y)\n",
        "    cost = loss(logit, y)\n",
        "\n",
        "    optimizer.zero_grad() # gradient를 0으로 초기화\n",
        "    cost.backward() # cost를 미분\n",
        "    optimizer.step() # SGD로 gradient update\n",
        "\n",
        "    total_loss += cost.item()\n",
        "\n",
        "    pred = torch.round(prob).item()\n",
        "    count += (pred == y.item())\n",
        "\n",
        "  acc = count / len(data)\n",
        "  loss_list.append(total_loss)\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(\"Epoch %d: Loss %.3f / Acc %.3f\" %(i, total_loss, acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClzF9321sZT5",
        "outputId": "ecef4126-a482-4ff7-9142-708e6cbacda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 591.218 / Acc 0.695\n",
            "Epoch 10: Loss 441.412 / Acc 0.665\n",
            "Epoch 20: Loss 431.252 / Acc 0.623\n",
            "Epoch 30: Loss 430.391 / Acc 0.672\n",
            "Epoch 40: Loss 429.751 / Acc 0.668\n",
            "Epoch 50: Loss 424.906 / Acc 0.680\n",
            "Epoch 60: Loss 431.054 / Acc 0.712\n",
            "Epoch 70: Loss 429.694 / Acc 0.671\n",
            "Epoch 80: Loss 426.548 / Acc 0.675\n",
            "Epoch 90: Loss 426.553 / Acc 0.691\n",
            "Epoch 100: Loss 429.623 / Acc 0.707\n",
            "Epoch 110: Loss 423.722 / Acc 0.716\n",
            "Epoch 120: Loss 421.024 / Acc 0.673\n",
            "Epoch 130: Loss 423.879 / Acc 0.702\n",
            "Epoch 140: Loss 422.192 / Acc 0.718\n",
            "Epoch 150: Loss 421.226 / Acc 0.733\n",
            "Epoch 160: Loss 424.510 / Acc 0.722\n",
            "Epoch 170: Loss 422.459 / Acc 0.698\n",
            "Epoch 180: Loss 423.448 / Acc 0.740\n",
            "Epoch 190: Loss 244.321 / Acc 0.925\n"
          ]
        }
      ]
    }
  ]
}