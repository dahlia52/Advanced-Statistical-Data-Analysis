{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahlia52/Advanced-Statistical-Data-Analysis/blob/main/softmax_classification_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFZrRhaNRqOv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liUistX7Bwv1",
        "outputId": "ec0c2ab5-98aa-491c-f4d4-0b3a1431a69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "784 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()]) # 이미지를 텐서로 변환\n",
        "\n",
        "# Prepare Data\n",
        "train_data = MNIST(root = path, train = True, transform = transform, download = True)\n",
        "test_data = MNIST(root = path, train = False, transform = transform, download = True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size = batch_size, shuffle = False, num_workers = 4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)\n",
        "\n",
        "input_shape = train_data[0][0].reshape(-1).shape[0] # 28*28 = 784 features\n",
        "output_shape = len(train_data.classes)\n",
        "\n",
        "print(input_shape,output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvmVJ0hxjkLD",
        "outputId": "5e040977-0d2b-4df0-b69f-cbdcfa7a49c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu:0\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfQrmlOhjUqH"
      },
      "source": [
        "## Experiment1 : SGD optimizer 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4wN7GgWRmFl"
      },
      "outputs": [],
      "source": [
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.linear1 = nn.Linear(input_shape, 256)\n",
        "        # self.linear2 = nn.Linear(256, 256)\n",
        "        # self.linear3 = nn.Linear(256, 256)\n",
        "        # self.linear4 = nn.Linear(256, output_shape)\n",
        "\n",
        "        # 1*28*28 -> 784\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_shape, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(128, output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # hidden = F.leaky_relu(self.linear1(x))\n",
        "        # hidden = F.leaky_relu(self.linear2(hidden))\n",
        "        # hidden = F.leaky_relu(self.linear3(hidden))\n",
        "        # output = self.linear4(hidden)\n",
        "\n",
        "        # return output\n",
        "\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqbuuetbRvAp"
      },
      "outputs": [],
      "source": [
        "model = SoftmaxClassifier().to(device)\n",
        "# logit = pre-softmax value = pre prob value = [0.9, 0.05, 0.02, ...] vs 0 = label value\n",
        "loss = nn.CrossEntropyLoss(reduction = 'sum') # minibatch 안의 100개의 loss를 모두 더함. 100 losses -> sum of 100 losses\n",
        "\n",
        "# minimize loss\n",
        "# minimize loss' = loss + weight_decay * l2_regularization\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgDxNECgTxyq",
        "outputId": "2f53f75c-a273-493a-8aa5-5368b111c040"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0 Train: Loss 0.777 / Accuracy 0.770\n",
            "Epoch 0 Test: Loss 7.848 / Accuracy 0.918\n",
            "\n",
            "Epoch 1 Train: Loss 0.207 / Accuracy 0.939\n",
            "Epoch 1 Test: Loss 8.536 / Accuracy 0.947\n",
            "\n",
            "Epoch 2 Train: Loss 0.132 / Accuracy 0.961\n",
            "Epoch 2 Test: Loss 9.992 / Accuracy 0.964\n",
            "\n",
            "Epoch 3 Train: Loss 0.095 / Accuracy 0.972\n",
            "Epoch 3 Test: Loss 11.483 / Accuracy 0.972\n",
            "\n",
            "Epoch 4 Train: Loss 0.075 / Accuracy 0.978\n",
            "Epoch 4 Test: Loss 11.486 / Accuracy 0.973\n",
            "\n",
            "Epoch 5 Train: Loss 0.059 / Accuracy 0.983\n",
            "Epoch 5 Test: Loss 12.593 / Accuracy 0.978\n",
            "\n",
            "Epoch 6 Train: Loss 0.048 / Accuracy 0.986\n",
            "Epoch 6 Test: Loss 12.784 / Accuracy 0.972\n",
            "\n",
            "Epoch 7 Train: Loss 0.039 / Accuracy 0.988\n",
            "Epoch 7 Test: Loss 13.683 / Accuracy 0.979\n",
            "\n",
            "Epoch 8 Train: Loss 0.031 / Accuracy 0.991\n",
            "Epoch 8 Test: Loss 14.669 / Accuracy 0.979\n",
            "\n",
            "Epoch 9 Train: Loss 0.025 / Accuracy 0.993\n",
            "Epoch 9 Test: Loss 15.751 / Accuracy 0.980\n",
            "\n",
            "Epoch 10 Train: Loss 0.020 / Accuracy 0.995\n",
            "Epoch 10 Test: Loss 16.674 / Accuracy 0.980\n",
            "\n",
            "Epoch 11 Train: Loss 0.015 / Accuracy 0.996\n",
            "Epoch 11 Test: Loss 17.654 / Accuracy 0.981\n",
            "\n",
            "Epoch 12 Train: Loss 0.013 / Accuracy 0.997\n",
            "Epoch 12 Test: Loss 16.411 / Accuracy 0.982\n",
            "\n",
            "Epoch 13 Train: Loss 0.010 / Accuracy 0.997\n",
            "Epoch 13 Test: Loss 18.669 / Accuracy 0.981\n",
            "\n",
            "Epoch 14 Train: Loss 0.007 / Accuracy 0.999\n",
            "Epoch 14 Test: Loss 19.789 / Accuracy 0.981\n",
            "\n",
            "Epoch 15 Train: Loss 0.005 / Accuracy 0.999\n",
            "Epoch 15 Test: Loss 19.714 / Accuracy 0.982\n",
            "\n",
            "Epoch 16 Train: Loss 0.003 / Accuracy 1.000\n",
            "Epoch 16 Test: Loss 21.389 / Accuracy 0.983\n",
            "\n",
            "Epoch 17 Train: Loss 0.002 / Accuracy 1.000\n",
            "Epoch 17 Test: Loss 21.284 / Accuracy 0.983\n",
            "\n",
            "Epoch 18 Train: Loss 0.002 / Accuracy 1.000\n",
            "Epoch 18 Test: Loss 22.654 / Accuracy 0.983\n",
            "\n",
            "Epoch 19 Train: Loss 0.002 / Accuracy 1.000\n",
            "Epoch 19 Test: Loss 22.218 / Accuracy 0.983\n",
            "\n",
            "Epoch 20 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 20 Test: Loss 23.711 / Accuracy 0.982\n",
            "\n",
            "Epoch 21 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 21 Test: Loss 23.429 / Accuracy 0.983\n",
            "\n",
            "Epoch 22 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 22 Test: Loss 23.813 / Accuracy 0.982\n",
            "\n",
            "Epoch 23 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 23 Test: Loss 25.759 / Accuracy 0.982\n",
            "\n",
            "Epoch 24 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 24 Test: Loss 23.744 / Accuracy 0.983\n",
            "\n",
            "Epoch 25 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 25 Test: Loss 23.967 / Accuracy 0.983\n",
            "\n",
            "Epoch 26 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 26 Test: Loss 25.615 / Accuracy 0.983\n",
            "\n",
            "Epoch 27 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 27 Test: Loss 25.100 / Accuracy 0.983\n",
            "\n",
            "Epoch 28 Train: Loss 0.001 / Accuracy 1.000\n",
            "Epoch 28 Test: Loss 25.077 / Accuracy 0.983\n",
            "\n",
            "Epoch 29 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 29 Test: Loss 25.180 / Accuracy 0.982\n",
            "\n",
            "Epoch 30 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 30 Test: Loss 26.565 / Accuracy 0.983\n",
            "\n",
            "Epoch 31 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 31 Test: Loss 26.394 / Accuracy 0.983\n",
            "\n",
            "Epoch 32 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 32 Test: Loss 28.696 / Accuracy 0.983\n",
            "\n",
            "Epoch 33 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 33 Test: Loss 26.720 / Accuracy 0.983\n",
            "\n",
            "Epoch 34 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 34 Test: Loss 27.891 / Accuracy 0.983\n",
            "\n",
            "Epoch 35 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 35 Test: Loss 28.624 / Accuracy 0.983\n",
            "\n",
            "Epoch 36 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 36 Test: Loss 26.310 / Accuracy 0.983\n",
            "\n",
            "Epoch 37 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 37 Test: Loss 27.352 / Accuracy 0.983\n",
            "\n",
            "Epoch 38 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 38 Test: Loss 27.528 / Accuracy 0.982\n",
            "\n",
            "Epoch 39 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 39 Test: Loss 28.336 / Accuracy 0.982\n",
            "\n",
            "Epoch 40 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 40 Test: Loss 29.205 / Accuracy 0.983\n",
            "\n",
            "Epoch 41 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 41 Test: Loss 29.351 / Accuracy 0.983\n",
            "\n",
            "Epoch 42 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 42 Test: Loss 28.820 / Accuracy 0.983\n",
            "\n",
            "Epoch 43 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 43 Test: Loss 27.663 / Accuracy 0.983\n",
            "\n",
            "Epoch 44 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 44 Test: Loss 28.497 / Accuracy 0.982\n",
            "\n",
            "Epoch 45 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 45 Test: Loss 27.709 / Accuracy 0.983\n",
            "\n",
            "Epoch 46 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 46 Test: Loss 28.472 / Accuracy 0.983\n",
            "\n",
            "Epoch 47 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 47 Test: Loss 28.794 / Accuracy 0.983\n",
            "\n",
            "Epoch 48 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 48 Test: Loss 28.368 / Accuracy 0.983\n",
            "\n",
            "Epoch 49 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 49 Test: Loss 28.379 / Accuracy 0.983\n",
            "\n",
            "Epoch 50 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 50 Test: Loss 28.166 / Accuracy 0.983\n",
            "\n",
            "Epoch 51 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 51 Test: Loss 30.264 / Accuracy 0.983\n",
            "\n",
            "Epoch 52 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 52 Test: Loss 28.969 / Accuracy 0.983\n",
            "\n",
            "Epoch 53 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 53 Test: Loss 29.858 / Accuracy 0.983\n",
            "\n",
            "Epoch 54 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 54 Test: Loss 29.429 / Accuracy 0.983\n",
            "\n",
            "Epoch 55 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 55 Test: Loss 30.622 / Accuracy 0.983\n",
            "\n",
            "Epoch 56 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 56 Test: Loss 30.791 / Accuracy 0.983\n",
            "\n",
            "Epoch 57 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 57 Test: Loss 31.326 / Accuracy 0.983\n",
            "\n",
            "Epoch 58 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 58 Test: Loss 30.132 / Accuracy 0.983\n",
            "\n",
            "Epoch 59 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 59 Test: Loss 30.264 / Accuracy 0.983\n",
            "\n",
            "Epoch 60 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 60 Test: Loss 29.585 / Accuracy 0.983\n",
            "\n",
            "Epoch 61 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 61 Test: Loss 29.324 / Accuracy 0.983\n",
            "\n",
            "Epoch 62 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 62 Test: Loss 30.489 / Accuracy 0.983\n",
            "\n",
            "Epoch 63 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 63 Test: Loss 30.850 / Accuracy 0.983\n",
            "\n",
            "Epoch 64 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 64 Test: Loss 31.316 / Accuracy 0.982\n",
            "\n",
            "Epoch 65 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 65 Test: Loss 31.809 / Accuracy 0.983\n",
            "\n",
            "Epoch 66 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 66 Test: Loss 29.904 / Accuracy 0.983\n",
            "\n",
            "Epoch 67 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 67 Test: Loss 30.932 / Accuracy 0.983\n",
            "\n",
            "Epoch 68 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 68 Test: Loss 30.836 / Accuracy 0.983\n",
            "\n",
            "Epoch 69 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 69 Test: Loss 30.415 / Accuracy 0.983\n",
            "\n",
            "Epoch 70 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 70 Test: Loss 32.665 / Accuracy 0.983\n",
            "\n",
            "Epoch 71 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 71 Test: Loss 30.963 / Accuracy 0.983\n",
            "\n",
            "Epoch 72 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 72 Test: Loss 30.481 / Accuracy 0.983\n",
            "\n",
            "Epoch 73 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 73 Test: Loss 31.673 / Accuracy 0.983\n",
            "\n",
            "Epoch 74 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 74 Test: Loss 30.273 / Accuracy 0.983\n",
            "\n",
            "Epoch 75 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 75 Test: Loss 31.620 / Accuracy 0.983\n",
            "\n",
            "Epoch 76 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 76 Test: Loss 31.465 / Accuracy 0.983\n",
            "\n",
            "Epoch 77 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 77 Test: Loss 32.579 / Accuracy 0.983\n",
            "\n",
            "Epoch 78 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 78 Test: Loss 31.622 / Accuracy 0.983\n",
            "\n",
            "Epoch 79 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 79 Test: Loss 32.458 / Accuracy 0.983\n",
            "\n",
            "Epoch 80 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 80 Test: Loss 32.322 / Accuracy 0.982\n",
            "\n",
            "Epoch 81 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 81 Test: Loss 31.948 / Accuracy 0.983\n",
            "\n",
            "Epoch 82 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 82 Test: Loss 32.165 / Accuracy 0.983\n",
            "\n",
            "Epoch 83 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 83 Test: Loss 32.614 / Accuracy 0.983\n",
            "\n",
            "Epoch 84 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 84 Test: Loss 31.196 / Accuracy 0.983\n",
            "\n",
            "Epoch 85 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 85 Test: Loss 30.647 / Accuracy 0.983\n",
            "\n",
            "Epoch 86 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 86 Test: Loss 32.587 / Accuracy 0.982\n",
            "\n",
            "Epoch 87 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 87 Test: Loss 31.578 / Accuracy 0.983\n",
            "\n",
            "Epoch 88 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 88 Test: Loss 31.709 / Accuracy 0.983\n",
            "\n",
            "Epoch 89 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 89 Test: Loss 32.739 / Accuracy 0.983\n",
            "\n",
            "Epoch 90 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 90 Test: Loss 33.041 / Accuracy 0.982\n",
            "\n",
            "Epoch 91 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 91 Test: Loss 30.242 / Accuracy 0.983\n",
            "\n",
            "Epoch 92 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 92 Test: Loss 34.693 / Accuracy 0.982\n",
            "\n",
            "Epoch 93 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 93 Test: Loss 32.492 / Accuracy 0.983\n",
            "\n",
            "Epoch 94 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 94 Test: Loss 31.470 / Accuracy 0.982\n",
            "\n",
            "Epoch 95 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 95 Test: Loss 32.132 / Accuracy 0.983\n",
            "\n",
            "Epoch 96 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 96 Test: Loss 32.376 / Accuracy 0.982\n",
            "\n",
            "Epoch 97 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 97 Test: Loss 30.923 / Accuracy 0.983\n",
            "\n",
            "Epoch 98 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 98 Test: Loss 31.792 / Accuracy 0.983\n",
            "\n",
            "Epoch 99 Train: Loss 0.000 / Accuracy 1.000\n",
            "Epoch 99 Test: Loss 33.160 / Accuracy 0.982\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "  # train\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(train_loader):\n",
        "    # MNIST: x.shape = (100,1,28,28) -> x.shape = (100,784), y.shape = (100,1) = (100,)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # 001000000 x\n",
        "    # 3\n",
        "    pred_y_est = model.forward(x)\n",
        "    cost = loss(pred_y_est, y) # loss 함수가 label을 one-hot vector로 바꾸어 pred_y_est와의 loss를 계산해줌.\n",
        "    y_est = F.softmax(pred_y_est,dim = -1) # 마지막 차원에 대해 softmax\n",
        "\n",
        "    total_loss += cost.item()\n",
        "\n",
        "    # L1 norm 적용 시,\n",
        "    # for param in model.parameters():\n",
        "    #   cost += torch.norm(param, 1)*0.001\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = torch.argmax(y_est, dim = -1)\n",
        "    count += (pred == y).sum().item()\n",
        "\n",
        "  acc = count / len(train_data)\n",
        "  avg_loss = total_loss / len(train_data)\n",
        "\n",
        "  if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  with torch.no_grad(): # gradient 계산X\n",
        "    for batch_idx, (x,y) in enumerate(test_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      pre_y_est = model.forward(x)\n",
        "      cost = loss(pred_y_est, y)\n",
        "      y_est = F.softmax(pre_y_est, dim = -1)\n",
        "\n",
        "      total_loss += cost.item()\n",
        "\n",
        "      pred = torch.argmax(y_est, dim = -1)\n",
        "      count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count / len(test_data)\n",
        "    avg_loss = total_loss / len(test_data)\n",
        "\n",
        "    test_loss_list.append(avg_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      print(\"Epoch %d Test: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmyizA7alhXU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oske_6ftjhYg"
      },
      "outputs": [],
      "source": [
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRkJek-Gjab7"
      },
      "source": [
        "## Experiment 2 : Adam Optimizer 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRWG5wvpjc47"
      },
      "outputs": [],
      "source": [
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.linear1 = nn.Linear(input_shape, 256)\n",
        "        # self.linear2 = nn.Linear(256, 256)\n",
        "        # self.linear3 = nn.Linear(256, 256)\n",
        "        # self.linear4 = nn.Linear(256, output_shape)\n",
        "\n",
        "        # 1*28*28 -> 784\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_shape, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Linear(128, output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # hidden = F.leaky_relu(self.linear1(x))\n",
        "        # hidden = F.leaky_relu(self.linear2(hidden))\n",
        "        # hidden = F.leaky_relu(self.linear3(hidden))\n",
        "        # output = self.linear4(hidden)\n",
        "\n",
        "        # return output\n",
        "\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPso_EFrjc7u"
      },
      "outputs": [],
      "source": [
        "model = SoftmaxClassifier().to(device)\n",
        "# logit = pre-softmax value = pre prob value = [0.9, 0.05, 0.02, ...] vs 0 = label value\n",
        "loss = nn.CrossEntropyLoss(reduction = 'sum') # minibatch 안의 100개의 loss를 모두 더함. 100 losses -> sum of 100 losses\n",
        "\n",
        "# minimize loss\n",
        "# minimize loss' = loss + weight_decay * l2_regularization\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRgoGa4RjdAr"
      },
      "outputs": [],
      "source": [
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "  # train\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(train_loader):\n",
        "    # MNIST: x.shape = (100,1,28,28) -> x.shape = (100,784), y.shape = (100,1) = (100,)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # 001000000 x\n",
        "    # 3\n",
        "    pred_y_est = model.forward(x)\n",
        "    cost = loss(pred_y_est, y) # loss 함수가 label을 one-hot vector로 바꾸어 pred_y_est와의 loss를 계산해줌.\n",
        "    y_est = F.softmax(pred_y_est,dim = -1) # 마지막 차원에 대해 softmax\n",
        "\n",
        "    total_loss += cost.item()\n",
        "\n",
        "    # L1 norm 적용 시,\n",
        "    # for param in model.parameters():\n",
        "    #   cost += torch.norm(param, 1)*0.001\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = torch.argmax(y_est, dim = -1)\n",
        "    count += (pred == y).sum().item()\n",
        "\n",
        "  acc = count / len(train_data)\n",
        "  avg_loss = total_loss / len(train_data)\n",
        "\n",
        "  if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  with torch.no_grad(): # gradient 계산X\n",
        "    for batch_idx, (x,y) in enumerate(test_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      pre_y_est = model.forward(x)\n",
        "      cost = loss(pred_y_est, y)\n",
        "      y_est = F.softmax(pre_y_est, dim = -1)\n",
        "\n",
        "      total_loss += cost.item()\n",
        "\n",
        "      pred = torch.argmax(y_est, dim = -1)\n",
        "      count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count / len(test_data)\n",
        "    avg_loss = total_loss / len(test_data)\n",
        "\n",
        "    test_loss_list.append(avg_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      print(\"Epoch %d Test: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3pEj1MNkWB0"
      },
      "source": [
        "## Experiment 3 : Adam Optimizer + Dropout 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztlQ_PrPUduV"
      },
      "outputs": [],
      "source": [
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.linear1 = nn.Linear(input_shape, 256)\n",
        "        # self.linear2 = nn.Linear(256, 256)\n",
        "        # self.linear3 = nn.Linear(256, 256)\n",
        "        # self.linear4 = nn.Linear(256, output_shape)\n",
        "\n",
        "        # 1*28*28 -> 784\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_shape, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(128, output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # hidden = F.leaky_relu(self.linear1(x))\n",
        "        # hidden = F.leaky_relu(self.linear2(hidden))\n",
        "        # hidden = F.leaky_relu(self.linear3(hidden))\n",
        "        # output = self.linear4(hidden)\n",
        "\n",
        "        # return output\n",
        "\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOQtgr69lARh"
      },
      "outputs": [],
      "source": [
        "model = SoftmaxClassifier().to(device)\n",
        "# logit = pre-softmax value = pre prob value = [0.9, 0.05, 0.02, ...] vs 0 = label value\n",
        "loss = nn.CrossEntropyLoss(reduction = 'sum') # minibatch 안의 100개의 loss를 모두 더함. 100 losses -> sum of 100 losses\n",
        "\n",
        "# minimize loss\n",
        "# minimize loss' = loss + weight_decay * l2_regularization\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjKTMChXlAUr"
      },
      "outputs": [],
      "source": [
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "  # train\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  for batch_idx, (x,y) in enumerate(train_loader):\n",
        "    # MNIST: x.shape = (100,1,28,28) -> x.shape = (100,784), y.shape = (100,1) = (100,)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # 001000000 x\n",
        "    # 3\n",
        "    pred_y_est = model.forward(x)\n",
        "    cost = loss(pred_y_est, y) # loss 함수가 label을 one-hot vector로 바꾸어 pred_y_est와의 loss를 계산해줌.\n",
        "    y_est = F.softmax(pred_y_est,dim = -1) # 마지막 차원에 대해 softmax\n",
        "\n",
        "    total_loss += cost.item()\n",
        "\n",
        "    # L1 norm 적용 시,\n",
        "    # for param in model.parameters():\n",
        "    #   cost += torch.norm(param, 1)*0.001\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = torch.argmax(y_est, dim = -1)\n",
        "    count += (pred == y).sum().item()\n",
        "\n",
        "  acc = count / len(train_data)\n",
        "  avg_loss = total_loss / len(train_data)\n",
        "\n",
        "  if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0\n",
        "  count = 0\n",
        "\n",
        "  with torch.no_grad(): # gradient 계산X\n",
        "    for batch_idx, (x,y) in enumerate(test_loader):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      pre_y_est = model.forward(x)\n",
        "      cost = loss(pred_y_est, y)\n",
        "      y_est = F.softmax(pre_y_est, dim = -1)\n",
        "\n",
        "      total_loss += cost.item()\n",
        "\n",
        "      pred = torch.argmax(y_est, dim = -1)\n",
        "      count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count / len(test_data)\n",
        "    avg_loss = total_loss / len(test_data)\n",
        "\n",
        "    test_loss_list.append(avg_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "      print(\"Epoch %d Test: Loss %.3f / Accuracy %.3f\"%(i,avg_loss,acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkGVKzLnlAXF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAlIGhwdlAZh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL4LoS8vi6x0"
      },
      "source": [
        "- MNIST 데이터셋은 너무 쉽기 때문에 어떤 모델이든 성능이 높게 나오는 경우가 많음.\n",
        "\n",
        "- without dropout : Train Loss는 빠르게 감소하지만, Test Loss는 줄어들다 어느 순간 증가함.\n",
        "\n",
        "- with dropout : Train Loss는 상대적으로 덜 빠르게 감소하지만, Test Loss도 같이 감소함. -> 모든 경우에 잘 작동하는 것은 아니지만, 과적합을 방지해줌."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6lTqWtaXfuFPcFcOsUXTq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}